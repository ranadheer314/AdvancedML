{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Accuracy isnt good as loss function??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is just a ratio of correctly classifying points in our training set. This metric is good and could be easily interpreted, but it has two large disadvantages. At first,  that we need a gradient to optimize our loss function effectively. And accuracy doesn't have gradients with respect to model parameters. So we cannot optimize it -- we cannot learn the model to accuracy score. And also, this model doesn't take into account the confidence of our model in this prediction. So actually, we have a dot product of weight vector and feature vector w and x, and the larger the score, the more the model is confident in this prediction. If this dot product has a positive sign and a large value, then the model is confident. But if the sign is positive, but the value is close to zero, then the model is inconfident. And we want not only a model that makes correct decisions -- that gets its classes -- but we want a confident model, and it's known from machine learning that models with high confidence generalize better. Okay. Accuracy doesn't fit, so we need some other loss function. May be we can use mean square error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
